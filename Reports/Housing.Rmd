
---
title: "Using Linear Regression to Predict House Prices in Washington"
author: "Katherine Botz, Jessica Páez Bonilla, Manuel Jordán Expósito"
output: 
  pdf_document: 
    latex_engine: xelatex
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Abstract
The aim of this Predictive Modeling Project is to find an accurate model for predicting house sale prices in King County, Washington. We build several multivariate linear regression models to predict the Price response variable and select the best fitting model. To do this, we split the dataset into Train and Test data, then train the model using the former and calculate the model accuracy using the latter. To find each model we first fit a Linear Regression model with all predictor variables, then perform a stepwise model selection by minimizing the Bayesian Information Criterion. To adhere to the assumptions of the model we perform nonlinear transformations on certain variables while also balancing the fit and accuracy of the model. We select the best model in the end by maximizing both the Adjusted R-Squared and calculated Accuracy of the Price prediction in the Test data. Our final model consists of 15 predictor variables (which are all significant based on the t-test p-values) giving an Adjusted R-Squared of 0.7687 and 87.47% Accuracy.

#Introduction
We use an open source dataset containing 21,613 instances of house sales in King County, Washington, USA between May 2014 and May 2015. The dataset includes descriptive information about the house itself (bedrooms, bathrooms, square feet, etc.), as well as location (in latitude and longitude, and zip code), grade, condition, and information about when it was built and/or renovated. With Price as the response variable, the goal is to build a Linear Regression Model to predict future prices of house sales based on these predictor variables.

#Methods
We first perform an exploratory descriptive analysis on the data, and by means of a correlation matrix we find out which variables may have more influence on the sale price of the houses (high correlation) and which ones do not have any predictive value (low correlation). The correlation coefficient measures the sample covariance on a scale from -1 to 1, where the a number closer to 1 indicates a strong positive linear correlation between the variables. Variables with significant correlation mean that it could have predictive capacity over the response variable. A Scatterplot Matrix is also a good tool to visualize this relationship between the indicator variables and the response variable.

After this preliminary analysis is done we use the `lm` function to fit different linear regression models, which approximates a linear relationship between the response variable, $Y$ (price), and the other predictor variables, $X_1,\ldots,X_p$, by finding the coefficients $\beta_0,\ldots,\beta_p$ such that: $Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_pX_p$

The linear model generated by `lm` is the one that minimizes the vertical distance of each data point projected on the line, or the least squares approximation. To select the best model we balance a few important qualities: the highest adjusted R-squared, significance of the variables, all while ensuring the normality assumption of the model is fulfilled. 

Multiple R-Squared is a measure of the fit of a model; the higher the R-Squared, the lower the average variance of the data around the multivariate linear model. However, the Multiple R-Squared increases with complexity of the model, so we use the Adjusted R-Squared which does not favor more complex models.

In addition to the Adjusted R-Squared, we analyze the significance of each predictor variable on the response variable. We measure the significance of each variable by the p-value of the t-test; the Null Hypothesis in this test is that each $\beta_j=0$. When the p-value is sufficiently low we can reject the Null Hypothesis and conclude that the variable has a significant effect on the response variable. A good model will have variables that all have a significant effect on $Y$.

To start with the model selection and analysis, we decide to use the stepAIC function to perform a stepwise model selection with different combinations of variables. This function analyzes each stepwise model for the best fit by finding the one with the smallest Bayesian Information Criterion when using k=log(n) as a parameter. This function (stepAIC) give us a set of variables with the most influence on the house price while minimizing the multivariate noise.

The Adjusted R-Squared and significance of variables are only a valid test of model fitness when the assumptions of the model are fulfilled. The assumptions of the model are: Linearity, Homoscedasticity, Normality, and Independence of the errors.  During the analysis we notice that the data may not have a Normal distribution for this model. To judge Normality, we plot the QQ-plot to check that the standardized residuals more or less follow the diagonal line. If there are extreme departures from the diagonal line, we know that there may not be Normality in the data. 

To fix the issue of Normality, we use the BoxCox function to find a nonlinear transformation and transform the variables that need the transformation to find a more normal distribution. However, we lose some fitness in our model by transforming too many variables. In the end, we balance model fitness in terms of Adjusted R-Squared and Normality and find a better model with a logarithmic transformation of Price and sqft_living.

The goal of fitting our model is to make accurate predictions of Price, so we select three final models and use the Test data to predict the Price and then compare to the actual values to calculate the model accuracy. In this way, based on accuracy, best fit, and significant variables we are able to select the best model.

#Statistical Analysis
We exclude the ID and Date variable, as these do not offer any predictive value for the price of the house. We also exclude sqft_basement, as it has a perfect dependent relationship with sqft_living. Finally we train the model with 80% of the data and reserve 20% to compare the predicted values to the actual values with our final selected model.

```{r include = FALSE}
library(rmarkdown)
library(MASS)
library(corrplot)
library(car)
library(nortest)
library(rmarkdown)
library(caret)
library(ggplot2)

housing.raw <- read.table(file = "kc_house_data.csv", header = TRUE, sep = ",")
summary(housing.raw)
housing <- housing.raw[,c(3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21)]

n.housing <- nrow(housing)
p.housing <- ncol(housing)

x <- model.matrix(price ~ ., data = housing)[,-1]
y <- housing$price

sample_size <- floor(0.80 * nrow(housing))
set.seed(3)
train_ind <- sample(seq_len(nrow(housing)), size = sample_size)

train <- housing[train_ind, ]
test <- housing[-train_ind, ]
```

We first obtain the sample Correlation Matrix to find the variables with the greatest Correlation with Price. We can inspect the Correlation Heat Map to visualize this correlation:

```{r, fig.width = 4.5, fig.align='center'}
cor.housing <- cor(housing)
corrplot(cor.housing,method="pie",tl.col = "sienna4",tl.srt = 45)
```

We find that the biggest Correlation Coefficients compared to Price are as follows:

| Variable | Correlation Coefficient |
|------:|:-----|
| sqft_living | 0.7020350 |
| grade | 0.6674342 |
| sqft_above | 0.6055673 |
| sqft_living15 | 0.5853789 |
| bathrooms | 0.5251375 |

: Top 5 Variables with Highest Correlation Coefficient Compared to Price

To further visualize the relationship between these particular variables and the response variable, we generate a Scatterplot Matrix:


```{r, fig.width = 4.5, fig.align='center'}
#scatterplotMatrix(~ price + sqft_living + grade + sqft_above + sqft_living15 + bathrooms, 
                  #reg.line = lm, smooth = FALSE, spread = FALSE, span = 0.5, ellipse = FALSE, 
                  #levels = c(.5, .9), id.n = 0, diagonal = 'density', data = housing)
```


Indeed there does seem to be a positive linear relationship between Price and these specific variables with the largest Correlation. However, it remains to be seen whether these variables will play a significant role in fitting a multivariate linear model to predict Price.  

To start, we will fit a linear model with all 18 variables with the response set to the logarithm of Price. We find that the Adjusted R-Squared is a strong 0.7672, but certain variables such as sqft_lot15 do not have a significant linear effect on Price, due to the large p-value of the t-test.

```{r}
modHouse1 <- lm(log(price) ~ ., data = train)
summary(modHouse1)
```


In order to find a better fitting model for Price, we perform a stepwise model selection with different combinations of variables using the stepAIC function. For this dataset, the stepAIC function finds a model with Adjusted R-Squared = 0.7671. Although this is marginally smaller than the model with all variables, the variables all seem to be a good predictor of our response variable Price, as shown by the respective t-test p-values.

```{r include = FALSE}
modBIC1 <- stepAIC(modHouse1, k = log(nrow(train)), trace = 0)
```

```{r}
summary(modBIC1)
```


Although this model does seem to be a good fit in terms of Adjusted R-Squared and t-test p-values, we find that we may not have Normality in the data. 

```{r, fig.width = 4.5, fig.align='center'}
qqPlot(modBIC1, main="QQ Plot")
```


This violates our assumptions for the model selection, so we will use the BoxCox function from the Caret library to find a proper transformation for each variables in order to come closer to Normality.

```{r include = FALSE}
caret::BoxCoxTrans(train$price) #log
caret::BoxCoxTrans(train$bathrooms)
caret::BoxCoxTrans(train$bedrooms)
caret::BoxCoxTrans(train$sqft_living) #log
caret::BoxCoxTrans(train$waterfront)
caret::BoxCoxTrans(train$view)
caret::BoxCoxTrans(train$condition) #1/sqrt(y)
caret::BoxCoxTrans(train$grade) #log
caret::BoxCoxTrans(train$sqft_above) #log
caret::BoxCoxTrans(train$yr_built) #y^2
caret::BoxCoxTrans(train$yr_renovated)
caret::BoxCoxTrans(train$zipcode) #1/y^2
caret::BoxCoxTrans(train$lat) #y^2
caret::BoxCoxTrans(train$long)
caret::BoxCoxTrans(train$sqft_living15) #log
```


```{r}
modBIC2 <- lm(formula = I(log(price)) ~ bedrooms + bathrooms + I(log(sqft_living)) + 
                sqft_lot + floors + waterfront + view + I(1/sqrt(condition)) + 
                I(log(grade)) + I((yr_built)^2) + yr_renovated + I(1/(zipcode)^2) + 
                I((lat)^2) + long + I(log(sqft_living15)), data = train)
summary(modBIC2)
```


The latest model has a lower Adjusted R-Squared of 0.7635, but the data distribution is getting further from Normality.

```{r, fig.width = 4.5, fig.align='center'}
qqPlot(modBIC2, main="QQ Plot")
```


With some trial and error using various combinations of nonlinear transformations, we find a better fitting model with Adjusted R-Squared = 0.7687 and with a more Normal distribution.

```{r}
modBIC3 <- lm(formula = log(price) ~ bedrooms + bathrooms + log(sqft_living) + 
     sqft_lot + floors + waterfront + view + condition + grade + 
     yr_built + yr_renovated + zipcode + lat + long + sqft_living15, data = train)
summary(modBIC3)
```

```{r, fig.width = 4.5, fig.align='center'}
qqPlot(modBIC3, main="QQ Plot")
```


The latest model has an improved Adjusted R-Squared with all significant variables, while also improving the Normal distribution and adhering to the assumptions of a multivariate linear model.  

We can analyze all models for best Adjusted R-Squared, Normality, and significant variables, but the model with the best fit will have the most accurate prediction of the response variable, Price. We will predict the Price using our reserved Test data for our top three models, then compare to the actual reported prices to compute the model accuracy.


```{r include = FALSE}
# Prediction of the response using the Test dataset 
price_prediction1 <- exp(predict(modBIC1, test))
price_prediction2 <- exp(predict(modBIC2, test))
price_prediction3 <- exp(predict(modBIC3, test))


# Find the accuracy of the predictions
actuals_predictions1 <- data.frame(cbind(test$price, price_prediction1))
actuals_predictions2 <- data.frame(cbind(test$price, price_prediction2))
actuals_predictions3 <- data.frame(cbind(test$price, price_prediction3))

accuracy1 <- cor(actuals_predictions1)
accuracy2 <- cor(actuals_predictions2)
accuracy3 <- cor(actuals_predictions3)

mape1 <- (1-mean(abs((actuals_predictions1$price_prediction1 - actuals_predictions1$V1))/actuals_predictions1$V1))
mape2 <- (1-mean(abs((actuals_predictions2$price_prediction2 - actuals_predictions2$V1))/actuals_predictions2$V1))
mape3 <- (1-mean(abs((actuals_predictions3$price_prediction3 - actuals_predictions3$V1))/actuals_predictions3$V1))

accuracy1
mape1

accuracy2
mape2

accuracy3
mape3

```

```{r include = FALSE}
vif(modBIC1)
vif(modBIC2)
vif(modBIC3)

AIC(modBIC1, k=log(n.housing))
AIC(modBIC2, k=log(n.housing))
AIC(modBIC3, k=log(n.housing))
```

```{r include = FALSE}
#Dimension Reduction 

housingPre <- subset(train, select = -c(price))

#Get the standardized PCA for the dataset
pcaHousingStd <- princomp(x = housingPre, cor = TRUE)
summary(pcaHousingStd) #explain 82% of variability with first 8 PCs

# Create a new dataset with the response + principal components
housingPCA <- data.frame("price" = train$price, pcaHousingStd$scores)

# Regression on all principal components > better than the original model but not a dimension reduction
modPCA <- lm(log(price) ~ ., data = housingPCA)
summary(modPCA) # adj R2: 0.7672
vif(modPCA)

# Regression on first 8 PCs, but not as good as with all PCs
modPCA2 <- lm(log(price) ~ Comp.1 + Comp.2 + Comp.3 + Comp.4 + Comp.5 + Comp.6 + Comp.7 + Comp.8, data = housingPCA)
summary(modPCA2) # adj R2: 0.7394

# StepAIC on the principal components > no better than the other models
modPCABIC <- stepAIC(modPCA, k = 2 * log(nrow(train)), trace = 0)
summary(modPCABIC) # adj R2: 0.767
vif(modPCABIC)
qqPlot(modPCABIC, main="QQ Plot")
```


| Model | Description |Adjusted R-Squared | Correlation | Accuracy
|:------|:-------|:-----:|:-----:|:-----:|
| modBIC1 | Model Selected by StepAIC | 0.7671 | 79.68% | 80.3%
| modBIC2 | Previous Model Modified by Transformations | 0.7635 | 86.95% | 80.4%
| modBIC3 | Previous Model With Limited Transformations | 0.7687 | 87.47% | 80.5%

: Top 3 Models for Fitness and Accuracy

*NOTE: Although there are 15 variables in each of the best models, an attempt to reduce the dimensions using Principal Component Analysis was not fruitful; the best model using Standardized Principal Components consisted of 16 elements. Therefore we will not include that analysis in this report.*

#Conclusions
Based on the previous table we come to the conclusion that the best model for predicting the house prices is the third one (modBIC3), as it has the highest Adjusted R-Squared (0.7687) and the highest accuracy rate (87.47%) in the prediction of the response variable.

In this model we have 15 variables that all significantly affect and offer predictive value to the house price. We use logarithmic transformations on the response variable and the variable sqft_living in the model. We also have normality in our data, therefore meeting the assumptions of the model and achieving the aim of our project to create a model to accurately predict Price. We can say with 87.47% certainty that this model will accurately predict the price of a house sold in King County, Washington, USA between May 2014 and May 2015 with the predictor variables in our model.

#References

Harlfoxem. "House Sales in King County, USA." *Kaggle*, 25 Aug. 2016, 
www.kaggle.com/harlfoxem/housesalesprediction.

Portugués, Eduardo García. *Notes for Predictive Modeling*. 29 Dec. 2017, bookdown.org/egarpor/PM-UC3M/.

Prabhakaran, Selva. "Tutorials on Advanced Stats and Machine Learning With R." *r-Statistics.co*, 2016, r-statistics.co.
